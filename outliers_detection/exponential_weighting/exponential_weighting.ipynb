{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import math as ma\n",
    "from matplotlib import pyplot as plt\n",
    "from astropy import constants\n",
    "from scipy import stats\n",
    "from astropy.io import fits\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, auc, precision_recall_curve\n",
    "import itertools\n",
    "from scipy.signal import savgol_filter, detrend\n",
    "from scipy.stats.mstats import winsorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = 128\n",
    "freqs = 640\n",
    "freqs_hf = 768\n",
    "part = 4\n",
    "\n",
    "data_directory = \"/Users/eormacstudio/Documents/20241116_observation_data_8s_before_with_cotter_time_corrections/uvfits/calibrated_fits/\"\n",
    "data_directory_after = \"/Users/eormacstudio/Documents/20241112_observation_data_8s_after_with_cotter_time_corrections/uvfits/calibrated_fits/\"\n",
    "data_directory_higher_frequency_2s = \"/Users/eormacstudio/Documents/20241123_contaminated_rfi_data_2s_with_no_flags/uvfits/calibrated_fits/\"\n",
    "data_directory_higher_frequency_8s = \"/Users/eormacstudio/Documents/20241119_contaminated_rfi_data_8s_with_no_flags/uvfits/calibrated_fits/\"\n",
    "data_directory_model_sn1 = \"/Users/eormacstudio/Documents/20240821_multiple_simulation_higher_thermal_noise_sn1/uvfits/calibrated_fits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import observation data\n",
    "\n",
    "observation_file = \"/Users/eormacstudio/Documents/winsorized_statistics/python_script/general_eor1_update_241010.csv\"\n",
    "\n",
    "df = pd.read_csv(observation_file, header=0, engine='python')\n",
    "df = df[['obs_id', 'groupid', 'starttime_utc', 'local_sidereal_time_deg', 'duration',\n",
    "        'int_time', 'freq_res', 'dataqualityname', 'bad_tiles', 'calibration',\n",
    "        'calibration_delays', 'center_frequency_mhz', 'channel_center_frequencies_mhz_csv',\n",
    "        'ra', 'ra_pointing', 'ra_phase_center', 'dec', 'dec_pointing', 'dec_phase_center',\n",
    "        'deleted_flag', 'good_tiles', 'mode', 'sky_temp', 'stoptime_utc', 'total_tiles', 'gridpoint_name', 'gridpoint_number']]\n",
    "df['date'] = df.starttime_utc.apply(lambda x: dt.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.000Z\").date())\n",
    "df['date_time'] = df.starttime_utc.apply(lambda x: dt.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.000Z\"))\n",
    "df['partition'] = pd.factorize(df['date'])[0] + 1\n",
    "#df = df[(df['partition'] == 1) & (df['gridpoint_number'] == 0)].reset_index()\n",
    "\n",
    "obs_list = df['obs_id'].to_list()\n",
    "\n",
    "polr = ['XX', 'XY', 'YX', 'YY']\n",
    "\n",
    "number_of_days = np.unique(df['partition'])\n",
    "gn = df['gridpoint_number'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by (number_days, number_gridpoint)\n",
    "grouped = df.groupby([\"partition\", \"gridpoint_number\"])[\"obs_id\"].apply(list).reset_index()\n",
    "\n",
    "# Prepare data for multiprocessing\n",
    "task_list = [(row[\"partition\"], row[\"gridpoint_number\"], row[\"obs_id\"]) for _, row in grouped.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(obs_days, grid, obs_list, data_directory) for obs_days, grid, obs_list in task_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import observation data higher\n",
    "\n",
    "observation_file = \"/Users/eormacstudio/Documents/winsorized_statistics/python_script/rfi_contaminated_data_eor1_2016.csv\"\n",
    "\n",
    "df_high = pd.read_csv(observation_file, header=0, engine='python')\n",
    "df_high = df_high[['obs_id', 'groupid', 'starttime_utc', 'local_sidereal_time_deg', 'duration',\n",
    "        'int_time', 'freq_res', 'dataqualityname', 'bad_tiles', 'calibration',\n",
    "        'calibration_delays', 'center_frequency_mhz', 'channel_center_frequencies_mhz_csv',\n",
    "        'ra', 'ra_pointing', 'ra_phase_center', 'dec', 'dec_pointing', 'dec_phase_center',\n",
    "        'deleted_flag', 'good_tiles', 'mode', 'sky_temp', 'stoptime_utc', 'total_tiles', 'gridpoint_name', 'gridpoint_number']]\n",
    "df_high['date'] = df_high.starttime_utc.apply(lambda x: dt.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.000Z\").date())\n",
    "df_high['date_time'] = df_high.starttime_utc.apply(lambda x: dt.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.000Z\"))\n",
    "df_high['partition'] = pd.factorize(df_high['date'])[0] + 1\n",
    "\n",
    "obs_list_high = df_high['obs_id'].to_list()\n",
    "\n",
    "polr = ['XX', 'XY', 'YX', 'YY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "for i in range(len(obs_list_high)):\n",
    "    f = fits.open(data_directory_higher_frequency_2s + \"hyperdrive_solutions_%s.fits\" %(obs_list_high[i]))\n",
    "\n",
    "    data = f['SOLUTIONS'].data[:, [76], [76], ::2]\n",
    "    #print(data.shape)\n",
    "    #data = data[:, :, :, 0]\n",
    "    data = data[:, :, 0]\n",
    "    data = np.nan_to_num(data, nan=np.nanmedian(data))\n",
    "\n",
    "    all_data.append(data)\n",
    "\n",
    "combined_array = np.concatenate(all_data, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_outliers(data, max_iters=100, tol=1e-6, epsilon=1e-8, k=3):\n",
    "    \"\"\"\n",
    "    Flags outliers in a 3D data array (time, frequency, antennas) using\n",
    "    exponential weighting for variance estimation.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 3D numpy array of shape (time, frequency, antennas).\n",
    "    - max_iters: int, maximum number of iterations for exponential weighting.\n",
    "    - tol: float, tolerance for convergence in exponential weighting.\n",
    "    - epsilon: float, minimum variance to avoid division by zero.\n",
    "    - k: float, threshold multiplier for outlier detection (default: 3-sigma rule).\n",
    "\n",
    "    Returns:\n",
    "    - flag_array: 3D numpy array (same shape as `data`) with 1 for outliers, 0 otherwise.\n",
    "    \"\"\"\n",
    "    time, antennas, freq = data.shape\n",
    "    flag_array = np.zeros((time, antennas, freq), dtype=int)  # Initialize flag array\n",
    "    outliers_data = []\n",
    "    all_mu = []\n",
    "    all_sigma_squared = []\n",
    "\n",
    "    # Iterate over frequency and antennas\n",
    "    for f in range(4): #freq):\n",
    "        for a in range(2): #antennas):\n",
    "            print(\"Process for: \", a, f)\n",
    "            \n",
    "            # Extract data for this frequency and antenna over time\n",
    "            time_series = data[:, a, f]\n",
    "\n",
    "            # Step 1: Apply exponential weighting algorithm\n",
    "            # Initialization\n",
    "\n",
    "            mu = np.median(time_series)\n",
    "            sigma_squared = np.var(time_series) if np.var(time_series) > epsilon else epsilon\n",
    "            \n",
    "            for _ in range(max_iters):\n",
    "                # Calculate weights\n",
    "                q = (time_series - mu)**2 / sigma_squared\n",
    "                weights = np.exp(-q / 4)\n",
    "                \n",
    "                # Update mean and variance\n",
    "                new_mu = np.sum(time_series * weights) / np.sum(weights)\n",
    "                new_sigma_squared = np.sum(((time_series - new_mu)**2 / sigma_squared - 2/3) * weights) / np.sum(weights)\n",
    "                new_sigma_squared *= sigma_squared\n",
    "                new_sigma_squared = max(new_sigma_squared, epsilon)\n",
    "\n",
    "                # Convergence check\n",
    "                if abs(new_mu - mu) < tol and abs(new_sigma_squared - sigma_squared) < tol:\n",
    "                    break\n",
    "\n",
    "                # Update for next iteration\n",
    "                mu, sigma_squared = new_mu, new_sigma_squared\n",
    "                all_mu.append(mu)\n",
    "                all_sigma_squared.append(sigma_squared)\n",
    "\n",
    "            # Step 2: Detect outliers\n",
    "            sigma = np.sqrt(sigma_squared)\n",
    "            outliers = np.abs(time_series - mu) > k * sigma\n",
    "            flag_array[:, a, f] = outliers.astype(int)  # Update flag array\n",
    "\n",
    "            outliers_data.append(outliers)\n",
    "\n",
    "    return flag_array, outliers_data, all_mu, all_sigma_squared\n",
    "\n",
    "# Flag outliers\n",
    "flag_array, od, all_mu, all_sigma_squared= flag_outliers(combined_array)\n",
    "\n",
    "# Find flagged locations\n",
    "flagged_locations = np.argwhere(flag_array == 1)\n",
    "print(f\"Flagged Locations (time, frequency, antenna):\\n{flagged_locations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged_locations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_array[flagged_locations[(flagged_locations[:,2] == f_idx) & (flagged_locations[:,1] == a_idx)][:,0], a_idx, f_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged_locations[flagged_locations[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_idx, f_idx = 4, 22  # Example frequency and antenna to visualize\n",
    "plt.plot(combined_array[:, a_idx, f_idx], label=f\"Freq={f_idx}, Antenna={a_idx}\")\n",
    "plt.scatter(\n",
    "    flagged_locations[flagged_locations[:, 2] == f_idx][:, 0],\n",
    "    combined_array[flagged_locations[flagged_locations[:, 2] == f_idx][:, 0], a_idx, f_idx],\n",
    "    color=\"red\", label=\"Outliers\"\n",
    ")\n",
    "plt.title(f\"Outlier Detection for Frequency={f_idx}, Antenna={a_idx}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_weighting_outlier_detection(data, max_iters=100, tol=1e-6, epsilon=1e-8, k=5):\n",
    "    \"\"\"\n",
    "    Implements the exponential weighting algorithm for robust variance estimation and outlier detection.\n",
    "\n",
    "    Parameters:\n",
    "    - data: array-like, the input data (e.g., real part of gain calibration solutions).\n",
    "    - max_iters: int, maximum number of iterations for convergence.\n",
    "    - tol: float, tolerance for convergence.\n",
    "    - epsilon: float, minimum variance to avoid division by zero.\n",
    "    - k: float, threshold for Z-score to classify outliers (default: 3-sigma rule).\n",
    "\n",
    "    Returns:\n",
    "    - mu_r: float, the robust mean.\n",
    "    - sigma_r_squared: float, the robust variance.\n",
    "    - z_scores: array-like, the Z-scores of the data points.\n",
    "    - outliers: array-like, indices of the detected outliers.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialization\n",
    "    mu_r = np.median(data)  # Robust initial guess for the mean\n",
    "    sigma_r_squared = np.var(data) if np.var(data) > epsilon else epsilon  # Initial variance\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        # Step 2: Compute weights based on current mean and variance\n",
    "        q = (data - mu_r)**2 / sigma_r_squared\n",
    "        weights = np.exp(-q / 4)\n",
    "\n",
    "        # Step 3: Update mean\n",
    "        new_mu_r = np.sum(data * weights) / np.sum(weights)\n",
    "\n",
    "        # Step 4: Update variance\n",
    "        new_sigma_r_squared = np.sum(((data - new_mu_r)**2 / sigma_r_squared - 2/3) * weights) / np.sum(weights)\n",
    "        new_sigma_r_squared *= sigma_r_squared\n",
    "        new_sigma_r_squared = max(new_sigma_r_squared, epsilon)  # Ensure variance is positive\n",
    "\n",
    "        # Step 5: Check for convergence\n",
    "        if abs(new_mu_r - mu_r) < tol and abs(new_sigma_r_squared - sigma_r_squared) < tol:\n",
    "            break\n",
    "\n",
    "        # Update for the next iteration\n",
    "        mu_r, sigma_r_squared = new_mu_r, new_sigma_r_squared\n",
    "\n",
    "    # Step 6: Compute Z-scores and identify outliers\n",
    "    sigma_r = np.sqrt(sigma_r_squared)\n",
    "    z_scores = (data - mu_r) / sigma_r\n",
    "    outliers = np.where(np.abs(z_scores) > k)[0]  # Indices of outliers\n",
    "\n",
    "    return mu_r, sigma_r_squared, z_scores, outliers\n",
    "\n",
    "# Example Data: Simulated data with outliers\n",
    "\n",
    "# Run the algorithm\n",
    "mu_r, sigma_r_squared, z_scores, outliers = exponential_weighting_outlier_detection(combined_array)\n",
    "\n",
    "# Print results\n",
    "print(\"Robust Mean:\", mu_r)\n",
    "print(\"Robust Variance:\", sigma_r_squared)\n",
    "print(\"Outlier Indices:\", len(outliers), outliers)\n",
    "print(\"Outlier Values:\", combined_array[outliers])\n",
    "print(\"Z_score:\", z_scores)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(combined_array, label=\"Data\", marker=\"o\", linestyle=\"-\")\n",
    "plt.scatter(outliers, combined_array[outliers], color=\"red\", label=\"Outliers\", zorder=5)\n",
    "plt.axhline(mu_r, color=\"green\", linestyle=\"--\", label=\"Robust Mean\")\n",
    "plt.fill_between(\n",
    "    range(len(combined_array)),\n",
    "    mu_r - 3 * np.sqrt(sigma_r_squared),\n",
    "    mu_r + 3 * np.sqrt(sigma_r_squared),\n",
    "    color=\"green\",\n",
    "    alpha=0.2,\n",
    "    label=\"3-Sigma Range\"\n",
    ")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Exponential Weighting: Outlier Detection\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possibly correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "for i in range(len(obs_list_high)):\n",
    "    f = fits.open(data_directory_higher_frequency_2s + \"hyperdrive_solutions_%s.fits\" %(obs_list_high[i]))\n",
    "\n",
    "    data = f['SOLUTIONS'].data[:, [20], [86], ::2]\n",
    "    #print(data.shape)\n",
    "    #data = data[:, :, :, 0]\n",
    "    data = data[:, :, 0]\n",
    "    data = np.nan_to_num(data, nan=np.nanmedian(data))\n",
    "\n",
    "    all_data.append(data)\n",
    "\n",
    "combined_array = np.concatenate(all_data, axis=0)\n",
    "\n",
    "\n",
    "def exponential_weighting_outlier_detection(data, max_iters=100, tol=1e-6, epsilon=1e-8, k=4):\n",
    "    \"\"\"\n",
    "    Implements the exponential weighting algorithm for robust variance estimation and outlier detection.\n",
    "\n",
    "    Parameters:\n",
    "    - data: array-like, the input data (e.g., real part of gain calibration solutions).\n",
    "    - max_iters: int, maximum number of iterations for convergence.\n",
    "    - tol: float, tolerance for convergence.\n",
    "    - epsilon: float, minimum variance to avoid division by zero.\n",
    "    - k: float, threshold for Z-score to classify outliers (default: 3-sigma rule).\n",
    "\n",
    "    Returns:\n",
    "    - mu_r: float, the robust mean.\n",
    "    - sigma_r_squared: float, the robust variance.\n",
    "    - z_scores: array-like, the Z-scores of the data points.\n",
    "    - outliers: array-like, indices of the detected outliers.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialization\n",
    "    mu_r = np.median(data)  # Robust initial guess for the mean\n",
    "    sigma_r_squared = np.var(data) if np.var(data) > epsilon else epsilon  # Initial variance\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        # Step 2: Compute weights based on current mean and variance\n",
    "        q = (data - mu_r)**2 / sigma_r_squared\n",
    "        weights = np.exp(-q / 4)\n",
    "\n",
    "        # Step 3: Update mean\n",
    "        new_mu_r = np.sum(data * weights) / np.sum(weights)\n",
    "\n",
    "        # Step 4: Update variance\n",
    "        new_sigma_r_squared = np.sum(((data - new_mu_r)**2 * weights) / np.sum(weights))\n",
    "        new_sigma_r_squared = max(new_sigma_r_squared, epsilon)  # Ensure variance is positive\n",
    "\n",
    "        # Step 5: Check for convergence\n",
    "        if abs(new_mu_r - mu_r) < tol and abs(new_sigma_r_squared - sigma_r_squared) < tol:\n",
    "            break\n",
    "\n",
    "        # Update for the next iteration\n",
    "        mu_r, sigma_r_squared = new_mu_r, new_sigma_r_squared\n",
    "\n",
    "    # Step 6: Compute Z-scores and identify outliers\n",
    "    sigma_r = np.sqrt(sigma_r_squared)\n",
    "    z_scores = (data - mu_r) / sigma_r\n",
    "    outliers = np.where(np.abs(z_scores) > k)[0]  # Indices of outliers\n",
    "\n",
    "    return mu_r, sigma_r_squared, z_scores, outliers\n",
    "\n",
    "# Example Data: Simulated data with outliers\n",
    "\n",
    "# Run the algorithm\n",
    "mu_r, sigma_r_squared, z_scores, outliers = exponential_weighting_outlier_detection(combined_array)\n",
    "\n",
    "# Print results\n",
    "print(\"Robust Mean:\", mu_r)\n",
    "print(\"Robust Variance:\", sigma_r_squared)\n",
    "print(\"Outlier Indices:\", outliers)\n",
    "print(\"Outlier Values:\", combined_array[outliers])\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(combined_array, label=\"Data\", marker=\"o\", linestyle=\"-\")\n",
    "plt.scatter(outliers, combined_array[outliers], color=\"red\", label=\"Outliers\", zorder=5)\n",
    "plt.axhline(mu_r, color=\"green\", linestyle=\"--\", label=\"Robust Mean\")\n",
    "plt.fill_between(\n",
    "    range(len(combined_array)),\n",
    "    mu_r - 4 * np.sqrt(sigma_r_squared),\n",
    "    mu_r + 4 * np.sqrt(sigma_r_squared),\n",
    "    color=\"green\",\n",
    "    alpha=0.2,\n",
    "    label=\"4-Sigma Range\"\n",
    ")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Exponential Weighting: Outlier Detection\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Proccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0\n",
    "number_of_timeblocks = 56\n",
    "all_data = []\n",
    "\n",
    "for i in range(len(obs_list)):\n",
    "    f = fits.open(data_directory_model_sn1 + \"hyperdrive_solutions_%s_noise.fits\" %(obs_list[i]))\n",
    "\n",
    "    data = f['SOLUTIONS'].data[:, :, :, ::2]\n",
    "    #data = data[:, :, :, :]\n",
    "\n",
    "    all_data.append(data)\n",
    "\n",
    "combined_array = np.concatenate(all_data, axis=0)\n",
    "\n",
    "\n",
    "def exponential_weighting_outlier_detection_3d(data, obs_list, number_of_timeblocks, max_iters=100, tol=1e-6, epsilon=1e-8, k=sigma):\n",
    "    \"\"\"\n",
    "    Implements the exponential weighting algorithm for robust variance estimation and outlier detection\n",
    "    for 3D data (time, antenna, frequency).\n",
    "\n",
    "    Parameters:\n",
    "    - data: 3D numpy array, input data of shape (time, antenna, frequency).\n",
    "    - max_iters: int, maximum number of iterations for convergence.\n",
    "    - tol: float, tolerance for convergence.\n",
    "    - epsilon: float, minimum variance to avoid division by zero.\n",
    "    - k: float, threshold for Z-score to classify outliers (default: 3-sigma rule).\n",
    "\n",
    "    Returns:\n",
    "    - flag_array: 3D numpy array of the same shape as `data`, where 1 indicates an outlier and 0 otherwise.\n",
    "    - stats: Dictionary containing robust mean and variance for each (antenna, frequency) combination.\n",
    "    \"\"\"\n",
    "    time, antennas, frequencies, polarizations = data.shape\n",
    "    flag_array = np.zeros_like(data, dtype=int)  # Initialize flag array\n",
    "    stats = {}  # To store mean and variance for each (antenna, frequency)\n",
    "    all_z_scores = {}\n",
    "    print(time, antennas, frequencies, polarizations)\n",
    "\n",
    "    # Iterate over antennas and frequencies\n",
    "    for polarization in range(polarizations):\n",
    "        for antenna in range(antennas):\n",
    "            for frequency in range(frequencies):\n",
    "                print(polarization, antenna, frequency)\n",
    "\n",
    "                # Extract the time series for the current antenna and frequency\n",
    "                time_series = data[:, antenna, frequency, polarization]\n",
    "                time_series = np.nan_to_num(time_series, nan=np.nanmedian(time_series))\n",
    "\n",
    "                # Step 1: Initialization\n",
    "                mu_r = np.median(time_series)  # Robust initial guess for the mean\n",
    "                sigma_r_squared = np.var(time_series) if np.var(time_series) > epsilon else epsilon  # Initial variance\n",
    "\n",
    "                for iteration in range(max_iters):\n",
    "                    # Step 2: Compute weights based on current mean and variance\n",
    "                    q = (time_series - mu_r)**2 / sigma_r_squared\n",
    "                    weights = np.exp(-q / 4)\n",
    "\n",
    "                    # Step 3: Update mean\n",
    "                    new_mu_r = np.sum(time_series * weights) / np.sum(weights)\n",
    "\n",
    "                    # Step 4: Update variance\n",
    "                    new_sigma_r_squared = np.sum(((time_series - new_mu_r)**2 * weights) / np.sum(weights))\n",
    "                    new_sigma_r_squared = max(new_sigma_r_squared, epsilon)  # Ensure variance is positive\n",
    "\n",
    "                    # Step 5: Check for convergence\n",
    "                    if abs(new_mu_r - mu_r) < tol and abs(new_sigma_r_squared - sigma_r_squared) < tol:\n",
    "                        break\n",
    "\n",
    "                    # Update for the next iteration\n",
    "                    mu_r, sigma_r_squared = new_mu_r, new_sigma_r_squared\n",
    "\n",
    "                # Store robust statistics\n",
    "                stats[(antenna, frequency, polarization)] = {'mean': mu_r, 'variance': sigma_r_squared}\n",
    "\n",
    "                # Step 6: Compute Z-scores and identify outliers\n",
    "                sigma_r = np.sqrt(sigma_r_squared)\n",
    "                z_scores = (time_series - mu_r) / sigma_r\n",
    "                outliers = np.abs(z_scores) > k\n",
    "                \n",
    "                all_z_scores[(antenna, frequency, polarization)] = {'data': time_series, 'z_score':z_scores, 'obs_id':np.repeat(obs_list, number_of_timeblocks, axis=0)}\n",
    "\n",
    "                # Update flag array\n",
    "                flag_array[:, antenna, frequency, polarization] = outliers.astype(int)\n",
    "\n",
    "    return flag_array, stats, all_z_scores\n",
    "\n",
    "# Run the algorithm\n",
    "flag_array, stats, all_z_scores = exponential_weighting_outlier_detection_3d(combined_array, obs_list, number_of_timeblocks)\n",
    "\n",
    "# Print results\n",
    "print(\"Flagged Outliers (time, antenna, frequency):\")\n",
    "outlier_indices = np.argwhere(flag_array == 1)\n",
    "#for idx in outlier_indices:\n",
    "    #print(f\"Time: {idx[0]}, Antenna: {idx[1]}, Frequency: {idx[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for a specific antenna and frequency\n",
    "antenna, frequency, polarization = 75, 700, 0\n",
    "plt.figure(figsize=(10, 6))\n",
    "time_series = combined_array[:, antenna, frequency, polarization]\n",
    "outliers = np.where(flag_array[:, antenna, frequency, polarization] == 1)[0]\n",
    "plt.plot(time_series, label=f\"Antenna {antenna}, Frequency {frequency}\", marker=\"o\")\n",
    "plt.scatter(outliers, time_series[outliers], color=\"red\", label=\"Outliers\", zorder=5)\n",
    "plt.axhline(stats[(antenna, frequency, polarization)]['mean'], color=\"green\", linestyle=\"--\", label=\"Robust Mean\")\n",
    "plt.fill_between(\n",
    "    range(len(time_series)),\n",
    "    stats[(antenna, frequency, polarization)]['mean'] - sigma * np.sqrt(stats[(antenna, frequency, polarization)]['variance']),\n",
    "    stats[(antenna, frequency, polarization)]['mean'] + sigma * np.sqrt(stats[(antenna, frequency, polarization)]['variance']),\n",
    "    color=\"green\",\n",
    "    alpha=0.2,\n",
    "    label=\"%s-Sigma Range\" %(sigma)\n",
    ")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(f\"Outlier Detection for Antenna {antenna}, Frequency {frequency}, Polarization {polarization}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_z_scores = pd.DataFrame(all_z_scores).T.reset_index()\n",
    "df_z_scores = df_z_scores.rename(columns={'level_0': 'tile', 'level_1': 'frequency', 'level_2': 'polarization'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier_statistics = pd.DataFrame(stats).T.reset_index()\n",
    "df_outlier_statistics = df_outlier_statistics.rename(columns={'level_0': 'tile', 'level_1': 'frequency', 'level_2': 'polarization'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs_id = pd.DataFrame(np.repeat(obs_list, 56, axis=0)).reset_index()\n",
    "df_obs_id = df_obs_id.rename(columns={0: 'obs_id'})\n",
    "\n",
    "df_outlier_indices = pd.DataFrame(outlier_indices)\n",
    "df_outlier_indices = df_outlier_indices.rename(columns={0: 'index', 1:'tile', 2:'frequency', 3:'polarization'})\n",
    "df_outlier_indices['obs_id'] = df_outlier_indices['index'].map(df_obs_id.set_index('index')['obs_id'].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_higher_2s = pd.DataFrame()\n",
    "\n",
    "for i in range(tiles):\n",
    "    print('Tile: ', i)\n",
    "    for j in range(7): #freqs_hf):\n",
    "\n",
    "        #print('Tile: ', i, ' and Freqs: ', j)\n",
    "\n",
    "        for k in range(len(obs_list_high)):\n",
    "            f = fits.open(data_directory_higher_frequency_2s + \"hyperdrive_solutions_%s.fits\" %(obs_list_high[k]))\n",
    "\n",
    "            data = f['SOLUTIONS'].data[:, i, j, ::2]\n",
    "\n",
    "            a = pd.DataFrame(data[:, [0,1,2,3]], columns=polr)\n",
    "            a['obs_id'] = obs_list_high[k]\n",
    "            a['timeblocks'] = range(0, len(a))\n",
    "            a['tile'] = i\n",
    "            a['freq_channels'] = j\n",
    "            df_data_higher_2s = pd.concat([df_data_higher_2s, a]).reset_index(drop = True)\n",
    "\n",
    "df_data_higher_2s['start_time'] = df_data_higher_2s['obs_id'].map(df_high.set_index('obs_id').to_dict()['date_time'])\n",
    "df_data_higher_2s['obs_time'] = df_data_higher_2s.apply(lambda x: x['start_time'] + pd.Timedelta(seconds=x['timeblocks'] * 2), axis=1)\n",
    "df_data_higher_2s['partition'] = df_data_higher_2s['obs_id'].map(df_high.set_index('obs_id').to_dict()['partition'])\n",
    "df_data_higher_2s['gridpoint_number'] = df_data_higher_2s['obs_id'].map(df_high.set_index('obs_id').to_dict()['gridpoint_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_higher_2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "56*128*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(df_data_higher_2s['XX']).reshape(56, 128, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_tile = df_data_higher_2s[df_data_higher_2s['tile'] == 27].reset_index(drop=True)\n",
    "df_data_tile = df_data_tile[~df_data_tile['XX'].isna()].reset_index(drop=True)\n",
    "df_data_tile = df_data_tile.sort_values('obs_time').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_data_tile['obs_time'], df_data_tile['XX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended_data = detrend(df_data_tile['XX'])\n",
    "\n",
    "winsorized_data = winsorize(detrended_data, limits=[0.05, 0.05])\n",
    "\n",
    "plt.plot(winsorized_data, label=\"Preprocessed Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(detrended_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flag_outliers(data, max_iters=100, tol=1e-6, epsilon=1e-8, k=3):\n",
    "    \"\"\"\n",
    "    Flags outliers in a 3D data array (time, frequency, antennas) using\n",
    "    exponential weighting for variance estimation.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 3D numpy array of shape (time, frequency, antennas).\n",
    "    - max_iters: int, maximum number of iterations for exponential weighting.\n",
    "    - tol: float, tolerance for convergence in exponential weighting.\n",
    "    - epsilon: float, minimum variance to avoid division by zero.\n",
    "    - k: float, threshold multiplier for outlier detection (default: 3-sigma rule).\n",
    "\n",
    "    Returns:\n",
    "    - flag_array: 3D numpy array (same shape as `data`) with 1 for outliers, 0 otherwise.\n",
    "    \"\"\"\n",
    "    time, freq, antennas = data.shape\n",
    "    flag_array = np.zeros((time, freq, antennas), dtype=int)  # Initialize flag array\n",
    "\n",
    "    # Iterate over frequency and antennas\n",
    "    for f in range(freq):\n",
    "        for a in range(antennas):\n",
    "            # Extract data for this frequency and antenna over time\n",
    "            time_series = data[:, f, a]\n",
    "\n",
    "            # Step 1: Apply exponential weighting algorithm\n",
    "            # Initialization\n",
    "            mu = np.median(time_series)\n",
    "            sigma_squared = np.var(time_series) if np.var(time_series) > epsilon else epsilon\n",
    "            \n",
    "            for _ in range(max_iters):\n",
    "                # Calculate weights\n",
    "                q = (time_series - mu)**2 / sigma_squared\n",
    "                weights = np.exp(-q / 4)\n",
    "                \n",
    "                # Update mean and variance\n",
    "                new_mu = np.sum(time_series * weights) / np.sum(weights)\n",
    "                new_sigma_squared = np.sum(((time_series - new_mu)**2 / sigma_squared - 2/3) * weights) / np.sum(weights)\n",
    "                new_sigma_squared *= sigma_squared\n",
    "                new_sigma_squared = max(new_sigma_squared, epsilon)\n",
    "\n",
    "                # Convergence check\n",
    "                if abs(new_mu - mu) < tol and abs(new_sigma_squared - sigma_squared) < tol:\n",
    "                    break\n",
    "\n",
    "                # Update for next iteration\n",
    "                mu, sigma_squared = new_mu, new_sigma_squared\n",
    "\n",
    "            # Step 2: Detect outliers\n",
    "            sigma = np.sqrt(sigma_squared)\n",
    "            outliers = np.abs(time_series - mu) > k * sigma\n",
    "            flag_array[:, f, a] = outliers.astype(int)  # Update flag array\n",
    "\n",
    "    return flag_array\n",
    "\n",
    "# Example 3D data: Simulated with random noise and outliers\n",
    "np.random.seed(42)\n",
    "time, freq, antennas = 100, 50, 5  # 100 time steps, 50 frequency channels, 5 antennas\n",
    "data = np.random.normal(1, 0.05, (time, freq, antennas))\n",
    "\n",
    "# Inject outliers\n",
    "data[20, 10, 2] += 3  # Outlier in (time=20, freq=10, antenna=2)\n",
    "data[50, 25, 1] -= 3  # Outlier in (time=50, freq=25, antenna=1)\n",
    "\n",
    "# Flag outliers\n",
    "flag_array = flag_outliers(data)\n",
    "\n",
    "# Find flagged locations\n",
    "flagged_locations = np.argwhere(flag_array == 1)\n",
    "print(f\"Flagged Locations (time, frequency, antenna):\\n{flagged_locations}\")\n",
    "\n",
    "# Visualize flags for a specific frequency channel and antenna\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f_idx, a_idx = 10, 2  # Example frequency and antenna to visualize\n",
    "plt.plot(data[:, f_idx, a_idx], label=f\"Freq={f_idx}, Antenna={a_idx}\")\n",
    "plt.scatter(\n",
    "    flagged_locations[flagged_locations[:, 1] == f_idx][:, 0],\n",
    "    data[flagged_locations[flagged_locations[:, 1] == f_idx][:, 0], f_idx, a_idx],\n",
    "    color=\"red\", label=\"Outliers\"\n",
    ")\n",
    "plt.title(f\"Outlier Detection for Frequency={f_idx}, Antenna={a_idx}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_observation_group(group, directory, number_of_timeblocks):\n",
    "    \"\"\"\n",
    "    Function to process a group of observations in parallel.\n",
    "    \"\"\"\n",
    "    number_days, number_gridpoint, observation_ids = group\n",
    "    print(f\"Processing: Days={number_days}, Gridpoints={number_gridpoint}, Observations={len(observation_ids)}\")\n",
    "    \n",
    "    # Simulate processing (Replace with actual logic)\n",
    "    for obs_id in observation_ids:\n",
    "        obs_path = os.path.join(directory, f\"{obs_id}.txt\")\n",
    "        print(f\"Processing {obs_path} with {number_of_timeblocks} timeblocks\")\n",
    "        \n",
    "    return f\"Completed: Days={number_days}, Gridpoints={number_gridpoint}\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to handle user input, load data, and process observations in parallel.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Process observation data in parallel.\")\n",
    "    parser.add_argument(\"-d\", \"--directory\", required=True, help=\"Directory containing observation data\")\n",
    "    parser.add_argument(\"-nt\", \"--number_of_timeblocks\", type=int, required=True, help=\"Number of timeblocks\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load observation ID dataframe (Simulated example)\n",
    "    data = {\n",
    "        \"observation_id\": [f\"obs_{i}\" for i in range(116)],\n",
    "        \"number_days\": [1, 2, 3, 4] * 29,\n",
    "        \"number_gridpoint\": [10, 20, 30] * 39\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Group by (number_days, number_gridpoint)\n",
    "    grouped = df.groupby([\"number_days\", \"number_gridpoint\"])[\"observation_id\"].apply(list).reset_index()\n",
    "    \n",
    "    # Prepare data for multiprocessing\n",
    "    task_list = [(row[\"number_days\"], row[\"number_gridpoint\"], row[\"observation_id\"]) for _, row in grouped.iterrows()]\n",
    "    \n",
    "    # Run multiprocessing\n",
    "    with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "        results = pool.starmap(process_observation_group, [(group, args.directory, args.number_of_timeblocks) for group in task_list])\n",
    "    \n",
    "    # Print results\n",
    "    for res in results:\n",
    "        print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "winsorizing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
